{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) Forklaring til hvad GridSearch er\n",
    "Den første opgave bestod af at forklare, hvad GridSearch helt konkret er. Der blev givet to kodeceller, hvortil den første skulle beskrives overfladisk, og den anden skulle beskrives mere dybdegående, ved de steder, hvor GridSearch fremtræder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK(function setup, hope MNIST loads works, seem best if you got Keras or Tensorflow installed!)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
    "            return ret_str          \n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print('OK(function setup, hope MNIST loads works, seem best if you got Keras or Tensorflow installed!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) Fortsat:\n",
    "`SearchReport` - tager en trænet model og printer parametrene fra den bedste model.<br>\n",
    "`ClassificationReport` - Udskriver information fra Scikit-learns `classification_report` funktion, og gør det ud fra testdataen. <br>\n",
    "`LoadAndSetupData` - indlæser dataen og og opdeler i train/test-splits. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285715\n",
      "\tbest index=2\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.962 (+/-0.093) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.371 (+/-0.038) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.971 (+/-0.047) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 3]: 0.695 (+/-0.047) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 4]: 0.952 (+/-0.085) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 5]: 0.924 (+/-0.097) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "SEARCH TIME: 4.10 sec\n",
      "CTOR for best model: SVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 2) the actual grid-search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(\n",
    "    gamma=0.001\n",
    ")  # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks,\n",
    "# FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel': ('linear', 'rbf'), \n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) Fortsat:\n",
    "Ud fra koden, kan det ses, at dataen først bliver loadet gennem funktionen `LoadAndSetupData`.<br>\n",
    "Derefter bliver algoritmen `SVC` (Support Vector Classification) valgt. I denne situation er der valgt ikke at prøve flere algoritmer, men det kunne gridsearch også have løst.<br>\n",
    "Efter dette bliver hyperparametrene bestemt, men modsat i de tidligere opgaver, bliver der nu oprettet en dictionary med lister/tuples af parametre. Dette indikerer hvilke hyperparametrer, som skal prøves i vores gridsearch. `GridSearchCV` modellen bliver derefter oprettet og fitted. Når en `GridSearchCV` model bliver fitted, er det lidt anderledes end det vi er vant til, for den forsøger at fitte med henblik på at finde de bedste hyperparametre. Den gør dette gennem en \"brute-force\" metode, hvor den prøver alle mulige kombinationer af de hyperparametre, som blev specificeret i vores dictionary.\n",
    "\n",
    "Hvad betyder det at der er blevet valgt scoringsmetoden `f1_micro`?<br>\n",
    "Det betyder at vi anvender en `f1_score` til at evaluere modellen men på \"micro\" niveau. Med dette menes at der kigges på alle klasser samlet i stedet for at score på de enkelte. Dette er fordelagtigt ved iris sættet, da vi har mange klasser, og ønsker at se den totale præstation (ikke med henblik på en enkelt label).\n",
    "\n",
    "\n",
    "Der blev valgt `n_jobs=-1` til algoritmen. Hvad betyder dette?<br>\n",
    "`n_jobs` angiver hvor mange tråde, der maksimalt skal dedikeres til at træne modellen. Når den sættes til -1, angiver det, at algoritmen skal bruge så mange tråde den kan. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Erstatning af SVC med SGD\n",
    "I denne opgave skal `SVC` algoritmen erstattes med en `SGD` algoritme. Der skal derefter foretages en gridsearch, som tager en ikke-ubetydelig mængde tid at eksekvere.\n",
    "\n",
    "Det ses af kodecellen nedenfor, at algoritmen er erstattet af en `SGDClassifier`. \n",
    "\n",
    "TODO: Skriv resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 'constant', 'penalty': 'l2'}\n",
      "\t[79842]: 0.486 (+/-0.378) for {'alpha': 1.0, 'eta0': 0.8080808080808082, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79843]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8080808080808082, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79844]: 0.562 (+/-0.279) for {'alpha': 1.0, 'eta0': 0.8080808080808082, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79845]: 0.686 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8080808080808082, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79846]: 0.657 (+/-0.140) for {'alpha': 1.0, 'eta0': 0.8080808080808082, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79847]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8080808080808082, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79848]: 0.371 (+/-0.251) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79849]: 0.333 (+/-0.085) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79850]: 0.390 (+/-0.140) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79851]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79852]: 0.619 (+/-0.335) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79853]: 0.648 (+/-0.214) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79854]: 0.571 (+/-0.269) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79855]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8181818181818182, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79856]: 0.400 (+/-0.222) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79857]: 0.352 (+/-0.076) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79858]: 0.400 (+/-0.230) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79859]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79860]: 0.543 (+/-0.344) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79861]: 0.676 (+/-0.338) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79862]: 0.590 (+/-0.293) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79863]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8282828282828284, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79864]: 0.314 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79865]: 0.352 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79866]: 0.400 (+/-0.316) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79867]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79868]: 0.581 (+/-0.304) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79869]: 0.695 (+/-0.214) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79870]: 0.581 (+/-0.279) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79871]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8383838383838385, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79872]: 0.324 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79873]: 0.324 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79874]: 0.571 (+/-0.289) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79875]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79876]: 0.676 (+/-0.038) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79877]: 0.752 (+/-0.203) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79878]: 0.467 (+/-0.368) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79879]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8484848484848485, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79880]: 0.362 (+/-0.177) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79881]: 0.314 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79882]: 0.562 (+/-0.279) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79883]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79884]: 0.571 (+/-0.301) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79885]: 0.752 (+/-0.251) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79886]: 0.514 (+/-0.321) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79887]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8585858585858587, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79888]: 0.324 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79889]: 0.324 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79890]: 0.486 (+/-0.348) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79891]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79892]: 0.438 (+/-0.194) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79893]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79894]: 0.457 (+/-0.293) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79895]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8686868686868687, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79896]: 0.448 (+/-0.333) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79897]: 0.324 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79898]: 0.600 (+/-0.273) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79899]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79900]: 0.667 (+/-0.104) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79901]: 0.667 (+/-0.085) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79902]: 0.486 (+/-0.378) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79903]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8787878787878789, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79904]: 0.324 (+/-0.038) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79905]: 0.343 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79906]: 0.629 (+/-0.212) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79907]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79908]: 0.524 (+/-0.263) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79909]: 0.648 (+/-0.143) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79910]: 0.457 (+/-0.344) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79911]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.888888888888889, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79912]: 0.467 (+/-0.251) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79913]: 0.333 (+/-0.060) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79914]: 0.571 (+/-0.209) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79915]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79916]: 0.552 (+/-0.311) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79917]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79918]: 0.533 (+/-0.368) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79919]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.8989898989898991, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79920]: 0.343 (+/-0.038) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79921]: 0.352 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79922]: 0.524 (+/-0.217) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79923]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79924]: 0.457 (+/-0.286) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79925]: 0.610 (+/-0.327) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79926]: 0.514 (+/-0.321) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79927]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9090909090909092, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79928]: 0.333 (+/-0.060) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79929]: 0.333 (+/-0.085) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79930]: 0.505 (+/-0.322) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79931]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79932]: 0.514 (+/-0.363) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79933]: 0.638 (+/-0.305) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79934]: 0.581 (+/-0.279) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79935]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9191919191919192, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79936]: 0.333 (+/-0.060) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79937]: 0.371 (+/-0.038) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79938]: 0.581 (+/-0.348) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79939]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79940]: 0.533 (+/-0.309) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79941]: 0.571 (+/-0.217) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79942]: 0.657 (+/-0.140) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79943]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9292929292929294, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79944]: 0.333 (+/-0.060) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79945]: 0.324 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79946]: 0.457 (+/-0.311) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79947]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79948]: 0.648 (+/-0.222) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79949]: 0.724 (+/-0.185) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79950]: 0.657 (+/-0.140) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79951]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9393939393939394, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79952]: 0.457 (+/-0.267) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79953]: 0.343 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79954]: 0.467 (+/-0.304) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79955]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79956]: 0.438 (+/-0.285) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79957]: 0.648 (+/-0.097) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79958]: 0.476 (+/-0.356) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79959]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9494949494949496, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79960]: 0.390 (+/-0.229) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79961]: 0.333 (+/-0.060) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79962]: 0.486 (+/-0.378) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79963]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79964]: 0.600 (+/-0.286) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79965]: 0.629 (+/-0.298) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79966]: 0.533 (+/-0.321) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79967]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9595959595959597, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79968]: 0.333 (+/-0.060) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79969]: 0.362 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79970]: 0.505 (+/-0.311) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79971]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79972]: 0.571 (+/-0.289) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79973]: 0.524 (+/-0.356) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79974]: 0.657 (+/-0.140) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79975]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9696969696969697, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79976]: 0.343 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79977]: 0.381 (+/-0.159) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79978]: 0.467 (+/-0.368) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79979]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79980]: 0.543 (+/-0.245) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79981]: 0.676 (+/-0.111) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79982]: 0.524 (+/-0.301) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79983]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.9797979797979799, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79984]: 0.324 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79985]: 0.343 (+/-0.071) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79986]: 0.533 (+/-0.368) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79987]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79988]: 0.505 (+/-0.322) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79989]: 0.619 (+/-0.256) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79990]: 0.457 (+/-0.293) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79991]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 0.98989898989899, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\t[79992]: 0.324 (+/-0.038) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'constant', 'penalty': 'l1'}\n",
      "\t[79993]: 0.343 (+/-0.071) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'constant', 'penalty': 'l2'}\n",
      "\t[79994]: 0.495 (+/-0.299) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'optimal', 'penalty': 'l1'}\n",
      "\t[79995]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'optimal', 'penalty': 'l2'}\n",
      "\t[79996]: 0.505 (+/-0.364) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'invscaling', 'penalty': 'l1'}\n",
      "\t[79997]: 0.657 (+/-0.093) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'invscaling', 'penalty': 'l2'}\n",
      "\t[79998]: 0.400 (+/-0.273) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'adaptive', 'penalty': 'l1'}\n",
      "\t[79999]: 0.695 (+/-0.047) for {'alpha': 1.0, 'eta0': 1.0, 'learning_rate': 'adaptive', 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "\n",
      "SEARCH TIME: 562.17 sec\n",
      "CTOR for best model: SGDClassifier(alpha=0.010101010101010102, eta0=0.36363636363636365,\n",
      "              penalty='l1')\n",
      "\n",
      "best: dat=iris, score=1.00000, model=SGDClassifier(alpha=0.010101010101010102,eta0=0.36363636363636365,learning_rate='optimal',penalty='l1')\n",
      "\n",
      "OK(grid-search)\n",
      "/home/morten/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "\n",
    "#def print_time():\n",
    "#    while(finished == False):\n",
    "#        print(f\"Time since start: {time() - start:.2f}\")\n",
    "#        sleep(10)\n",
    "        \n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = SGDClassifier()\n",
    "\n",
    "penalty = ('l1', 'l2')\n",
    "alpha = np.linspace(start=1E-20, stop=1, num=100)\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "eta0 = np.linspace(start=1E-20, stop=1, num=100)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'penalty': penalty, \n",
    "    'alpha': alpha,\n",
    "    'learning_rate': learning_rate,\n",
    "    'eta0': eta0\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 1\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "#finished = False\n",
    "#thread = Thread(target = print_time, args = [])\n",
    "#thread.start()\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "#finished = True\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc) Tilfældig søgning\n",
    "I denne opgave udskiftes `GridSearchCV` med den tilfældige søgningsfunktion `RandomizedSearchCV`. Målet er at tjekke om vi opnår at finde en ligestående model ud fra de samme hyperparametre. \n",
    "\n",
    "Først skal der gives en forklaring for den nye parametre `n_iter`, som bruges i `RandomizedSearchCV`. `n_iter` er antallet af forskellige kombinationer af de givede hyperparametre. Hvis antaller er 20, ses det at der dannes 20 forskellige modeller af hyperparametrene, som i dette tilfælde krydsvalideres 5 gange (`CV`). Dermed opnås der 100 fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "Time since start: 0.00\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'penalty': 'l1', 'learning_rate': 'optimal', 'eta0': 0.9655172413793103, 'alpha': 0.034482758620689655}\n",
      "\tbest 'f1_micro' score=0.9428571428571427\n",
      "\tbest index=8\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.034482758620689655, eta0=0.9655172413793103, penalty='l1')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.762 (+/-0.200) for {'penalty': 'l1', 'learning_rate': 'invscaling', 'eta0': 0.5862068965517241, 'alpha': 0.10344827586206896}\n",
      "\t[ 1]: 0.695 (+/-0.047) for {'penalty': 'l1', 'learning_rate': 'adaptive', 'eta0': 0.4482758620689655, 'alpha': 0.7586206896551724}\n",
      "\t[ 2]: 0.695 (+/-0.047) for {'penalty': 'l1', 'learning_rate': 'optimal', 'eta0': 0.7931034482758621, 'alpha': 0.7241379310344828}\n",
      "\t[ 3]: 0.695 (+/-0.047) for {'penalty': 'l2', 'learning_rate': 'adaptive', 'eta0': 0.6206896551724138, 'alpha': 0.7241379310344828}\n",
      "\t[ 4]: 0.695 (+/-0.047) for {'penalty': 'l1', 'learning_rate': 'invscaling', 'eta0': 0.7241379310344828, 'alpha': 0.5172413793103449}\n",
      "\t[ 5]: 0.619 (+/-0.289) for {'penalty': 'l1', 'learning_rate': 'invscaling', 'eta0': 0.896551724137931, 'alpha': 0.41379310344827586}\n",
      "\t[ 6]: 0.695 (+/-0.047) for {'penalty': 'l1', 'learning_rate': 'adaptive', 'eta0': 0.896551724137931, 'alpha': 0.7931034482758621}\n",
      "\t[ 7]: 0.448 (+/-0.311) for {'penalty': 'l2', 'learning_rate': 'constant', 'eta0': 0.10344827586206896, 'alpha': 0.896551724137931}\n",
      "\t[ 8]: 0.943 (+/-0.038) for {'penalty': 'l1', 'learning_rate': 'optimal', 'eta0': 0.9655172413793103, 'alpha': 0.034482758620689655}\n",
      "\t[ 9]: 0.695 (+/-0.047) for {'penalty': 'l1', 'learning_rate': 'adaptive', 'eta0': 0.20689655172413793, 'alpha': 0.7586206896551724}\n",
      "\t[10]: 0.695 (+/-0.047) for {'penalty': 'l1', 'learning_rate': 'optimal', 'eta0': 0.4482758620689655, 'alpha': 0.6206896551724138}\n",
      "\t[11]: 0.667 (+/-0.148) for {'penalty': 'l1', 'learning_rate': 'optimal', 'eta0': 0.24137931034482757, 'alpha': 0.7931034482758621}\n",
      "\t[12]: 0.695 (+/-0.047) for {'penalty': 'l2', 'learning_rate': 'adaptive', 'eta0': 0.9655172413793103, 'alpha': 0.8620689655172413}\n",
      "\t[13]: 0.695 (+/-0.047) for {'penalty': 'l1', 'learning_rate': 'invscaling', 'eta0': 0.3448275862068966, 'alpha': 0.48275862068965514}\n",
      "\t[14]: 0.695 (+/-0.047) for {'penalty': 'l2', 'learning_rate': 'optimal', 'eta0': 0.20689655172413793, 'alpha': 0.4482758620689655}\n",
      "\t[15]: 0.695 (+/-0.047) for {'penalty': 'l2', 'learning_rate': 'adaptive', 'eta0': 0.13793103448275862, 'alpha': 0.41379310344827586}\n",
      "\t[16]: 0.590 (+/-0.354) for {'penalty': 'l1', 'learning_rate': 'optimal', 'eta0': 0.5517241379310345, 'alpha': 1e-20}\n",
      "\t[17]: 0.371 (+/-0.038) for {'penalty': 'l2', 'learning_rate': 'invscaling', 'eta0': 1e-20, 'alpha': 0.24137931034482757}\n",
      "\t[18]: 0.552 (+/-0.328) for {'penalty': 'l2', 'learning_rate': 'constant', 'eta0': 0.20689655172413793, 'alpha': 0.10344827586206896}\n",
      "\t[19]: 0.733 (+/-0.097) for {'penalty': 'l2', 'learning_rate': 'adaptive', 'eta0': 0.9655172413793103, 'alpha': 0.3103448275862069}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "SEARCH TIME: 0.22 sec\n",
      "CTOR for best model: SGDClassifier(alpha=0.034482758620689655, eta0=0.9655172413793103, penalty='l1')\n",
      "\n",
      "best: dat=iris, score=0.94286, model=SGDClassifier(alpha=0.034482758620689655,eta0=0.9655172413793103,learning_rate='optimal',penalty='l1')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "\n",
    "def print_time():\n",
    "    while(finished == False):\n",
    "        print(f\"Time since start: {time() - start:.2f}\")\n",
    "        sleep(10)\n",
    "        \n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = SGDClassifier()\n",
    "\n",
    "penalty = ('l1', 'l2')\n",
    "alpha = np.linspace(start=1E-20, stop=1, num=30)\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "eta0 = np.linspace(start=1E-20, stop=1, num=30)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'penalty': penalty, \n",
    "    'alpha': alpha,\n",
    "    'learning_rate': learning_rate,\n",
    "    'eta0': eta0\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 1\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "finished = False\n",
    "thread = Thread(target = print_time, args = [])\n",
    "thread.start()\n",
    "grid_tuned = RandomizedSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          n_iter=20,\n",
    "                          random_state=42,\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "finished = True\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc) Fortsat\n",
    "Der skal sammenlignes mellem `GridSearchCV` og `RandomizedSearchCV` på både tid og score. \n",
    "\n",
    "Tidsmæssigt er der en væsentlig forskel. Den tidligere `GridSearchCV` tog 562.17 sekunder hvorimod `RandomizedSearchCV` kun tog 0.22 sekunder om at finde den bedst mulige model. I tilfælde at datasættet bliver scaleret op og der blivere søgt med flere parametre, vil der kunne spare yderligere tid ved søgning.\n",
    "\n",
    "Hvis man kigger på scoren af de to fundne modeller ved søgning er der ikke stor forskel. `GridSearchCV` fandt en model med en score på 1.00000 og `RandomizedSearchCV` fandt en model med en 0.94286 score. \n",
    "\n",
    "Ved kun at se på disse modeller vil det vurderes at `RandomSearchCV` ser ud til at give en fornuftig model på meget kort tid. Men dette er måske ikke den bedste usecase til at vise hvilken form for søgning er den bedste, da datasættet er forholdvist småt og parametrene få.\n",
    "\n",
    "Hvis datasættet er enormt og søgningen er med mange parametre, kunne der eventuelt bruges `RandomizedSearchCV` til at lave den første store grove søgning, hvorefter de bedste resultater af den søgning, kunne bruges til at lave en `GridSearchCV` og dermed nå frem til en god model og  endda spare tid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd) MNIST søgning\n",
    "\n",
    "Nu skal datasættet ændres til MNIST. Målet med denne opgave er at lave søgninger efter den bedste model til at prædikte MNIST-datasættet. \n",
    "\n",
    "Modellerne der valgt i søgningerne er `SDGClassifier`, `RandomForestClassifier` og `GaussianProcessClassifier`. Disse modeller har forskellige parametre som defineres i listen `parameters`. En for-løkke laver så søgninger med hver enkelt model og gemmer den bedste model i listen `b`, som til slut udskrives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'mnist')  # 'iris', 'moon', or 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: mnist..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\OneDrive - Aarhus Universitet\\MAL (Machine Learning)\\MAL\\ASS03\\libitmal\\dataloaders.py:65: UserWarning: MNIST_GetDataSet(): failed to import and load data in load_mode 'tensorflow.keras', proceding to next mode..\n",
      "  warnings.warn(\"MNIST_GetDataSet(): failed to import and load data in load_mode 'tensorflow.keras', proceding to next mode..\")\n",
      "C:\\Users\\thoma\\OneDrive - Aarhus Universitet\\MAL (Machine Learning)\\MAL\\ASS03\\libitmal\\dataloaders.py:77: UserWarning: MNIST_GetDataSet(): failed to import and load data in load_mode 'keras', proceding to next mode..\n",
      "  warnings.warn(\"MNIST_GetDataSet(): failed to import and load data in load_mode 'keras', proceding to next mode..\")\n",
      "C:\\Users\\thoma\\OneDrive - Aarhus Universitet\\MAL (Machine Learning)\\MAL\\ASS03\\libitmal\\dataloaders.py:88: UserWarning: MNIST_GetDataSet(): fetch openml mode is slow and uses 'float64' instead of 'uint8'\n",
      "  warnings.warn(\"MNIST_GetDataSet(): fetch openml mode is slow and uses 'float64' instead of 'uint8'\")\n",
      "C:\\Users\\thoma\\OneDrive - Aarhus Universitet\\MAL (Machine Learning)\\MAL\\ASS03\\libitmal\\dataloaders.py:92: UserWarning: MNIST_GetDataSet(): fetch openml mode converts y from '<class 'str'>' to 'uint8'\n",
      "  warnings.warn(f\"MNIST_GetDataSet(): fetch openml mode converts y from '{type(y[0])}' to 'uint8'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  org. data:  X.shape      =(70000;  784), y.shape      =(70000)\n",
      "  train data: X_train.shape=(49000;  784), y_train.shape=(49000)\n",
      "  test data:  X_test.shape =(21000;  784), y_test.shape =(21000)\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.0min remaining:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'penalty': 'l1', 'learning_rate': 'invscaling', 'eta0': 0.5862068965517241, 'alpha': 0.10344827586206896}\n",
      "\tbest 'f1_micro' score=0.8179183673469389\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.10344827586206896, eta0=0.5862068965517241,\n",
      "              learning_rate='invscaling', penalty='l1')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.818 (+/-0.043) for {'penalty': 'l1', 'learning_rate': 'invscaling', 'eta0': 0.5862068965517241, 'alpha': 0.10344827586206896}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      2077\n",
      "           1       0.91      0.94      0.92      2385\n",
      "           2       0.90      0.82      0.86      2115\n",
      "           3       0.87      0.82      0.84      2117\n",
      "           4       0.90      0.84      0.87      2004\n",
      "           5       0.79      0.79      0.79      1900\n",
      "           6       0.92      0.88      0.90      2045\n",
      "           7       0.68      0.95      0.79      2189\n",
      "           8       0.77      0.76      0.77      2042\n",
      "           9       0.86      0.71      0.78      2126\n",
      "\n",
      "    accuracy                           0.85     21000\n",
      "   macro avg       0.86      0.85      0.85     21000\n",
      "weighted avg       0.86      0.85      0.85     21000\n",
      "\n",
      "\n",
      "SEARCH TIME: 227.42 sec\n",
      "CTOR for best model: SGDClassifier(alpha=0.10344827586206896, eta0=0.5862068965517241,\n",
      "              learning_rate='invscaling', penalty='l1')\n",
      "\n",
      "best: dat=mnist, score=0.81792, model=SGDClassifier(alpha=0.10344827586206896,eta0=0.5862068965517241,learning_rate='invscaling',penalty='l1')\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    3.9s remaining:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'n_estimators': 37, 'min_weight_fraction_leaf': 0.4444444444444444, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_leaf_nodes': 187, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}\n",
      "\tbest 'f1_micro' score=0.6014081632653061\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tRandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
      "                       max_features='log2', max_leaf_nodes=187,\n",
      "                       min_samples_leaf=5, min_samples_split=7,\n",
      "                       min_weight_fraction_leaf=0.4444444444444444,\n",
      "                       n_estimators=37)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.601 (+/-0.049) for {'n_estimators': 37, 'min_weight_fraction_leaf': 0.4444444444444444, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_leaf_nodes': 187, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.90      0.65      2077\n",
      "           1       0.56      0.96      0.71      2385\n",
      "           2       0.74      0.39      0.51      2115\n",
      "           3       0.71      0.60      0.65      2117\n",
      "           4       0.50      0.79      0.61      2004\n",
      "           5       0.74      0.04      0.07      1900\n",
      "           6       0.66      0.53      0.59      2045\n",
      "           7       0.61      0.73      0.67      2189\n",
      "           8       0.61      0.60      0.60      2042\n",
      "           9       0.77      0.27      0.40      2126\n",
      "\n",
      "    accuracy                           0.59     21000\n",
      "   macro avg       0.64      0.58      0.55     21000\n",
      "weighted avg       0.64      0.59      0.55     21000\n",
      "\n",
      "\n",
      "SEARCH TIME: 6.28 sec\n",
      "CTOR for best model: RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
      "                       max_features='log2', max_leaf_nodes=187,\n",
      "                       min_samples_leaf=5, min_samples_split=7,\n",
      "                       min_weight_fraction_leaf=0.4444444444444444,\n",
      "                       n_estimators=37)\n",
      "\n",
      "best: dat=mnist, score=0.60141, model=RandomForestClassifier(class_weight='balanced_subsample',criterion='entropy',max_features='log2',max_leaf_nodes=187,min_samples_leaf=5,min_samples_split=7,min_weight_fraction_leaf=0.4444444444444444,n_estimators=37)\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.1min remaining:  3.1min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier # <- her\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier \n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Setup search parameters\n",
    "models = [SGDClassifier(), RandomForestClassifier(), GaussianProcessClassifier()]\n",
    "\n",
    "almost_zero_to_one=np.linspace(start=1E-20, stop=1, num=30)\n",
    "zero_to_one=np.linspace(start=0.0, stop=1.0, num=30)\n",
    "one_to_thousand = np.arange(1, 200)\n",
    "zero_to_ten = np.arange(0, 10)\n",
    "one_to_ten = np.arange(1,10)\n",
    "zero_to_half = np.linspace(start=0, stop=0.5, num=10)\n",
    "zero_to_hundred = np.arange(0,100)\n",
    "hundred_to_twohundred = np.arange(100,200)\n",
    "\n",
    "parameters=[\n",
    "    {\n",
    "        #SDG Classifier\n",
    "        'penalty': ('l1', 'l2'), \n",
    "        'alpha': almost_zero_to_one,\n",
    "        'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'eta0': almost_zero_to_one\n",
    "    },\n",
    "    {\n",
    "        #RandomForest\n",
    "        'n_estimators': one_to_thousand,\n",
    "        'criterion': ('gini','entropy'),\n",
    "        'min_samples_split': one_to_ten,\n",
    "        'min_samples_leaf': zero_to_ten,\n",
    "        'min_weight_fraction_leaf': zero_to_half,\n",
    "        'max_features': ('auto','sqrt','log2',None),\n",
    "        'max_leaf_nodes': [*one_to_thousand, None],\n",
    "        'class_weight': ('balanced','balanced_subsample')\n",
    "    },\n",
    "    {\n",
    "        #Gaussian Classifier\n",
    "        'warm_start': (True,False),\n",
    "        'n_restarts_optimizer': zero_to_hundred,\n",
    "        'max_iter_predict': hundred_to_twohundred,\n",
    "        'multi_class': ('one_vs_rest', 'one_vs_one')\n",
    "    }\n",
    "]\n",
    "\n",
    "# tupple\n",
    "searchables = (models,parameters)\n",
    "\n",
    "# Reports\n",
    "b = []\n",
    "m = []\n",
    "\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 1\n",
    "globalstart = time()\n",
    "for i in range(len(searchables[0])):\n",
    "    # Run search for the model\n",
    "    localstart = time()\n",
    "    grid_tuned = RandomizedSearchCV(searchables[0][i],\n",
    "                                  searchables[1][i],\n",
    "                                  cv=CV,\n",
    "                                  scoring='f1_micro',\n",
    "                                  n_iter=1,\n",
    "                                  random_state=42,\n",
    "                                  verbose=VERBOSE,\n",
    "                                  n_jobs=-1)\n",
    "    grid_tuned.fit(X_train, y_train)\n",
    "    t = time() - localstart\n",
    "    # Report result\n",
    "    b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "    b.append(b0)\n",
    "    m.append(m0)\n",
    "    \n",
    "t = time() - globalstart\n",
    "clear_output(wait=True)\n",
    "print(\"Tid: \" + str(t) +\"\\n\")\n",
    "print(\"SGDClassifier: \" + b[0] + \"\\n\\nRandomForest: \" + b[1] + \"\\n\\n Gaussian Classifier: \" + b[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
